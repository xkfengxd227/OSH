\documentclass{article} % For LaTeX2e
\usepackage{nips2016,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Fast Neighborhood Graph Converge based on Orientation Sensitive Hashing}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Converge on neighborhood graph tends to encounter a lot of disk page accesses. We present a technique called OSH (Orientation Sensitive Hashing) to prevent from wasting unnecessary disk page accesses. Inspired by LSH, we find that during the expansion, neighbors that are closer to the query are more like to provide valid converge path. However, most methods do not pay attention on filter out further neighbors which we consider as false positives (or unnecessary points) during the expansion.

OSH can register the orientation of all neighbors of a node. During the ANN search, the orientation of the query w.r.t the current node can also be figured out quickly. We then sort all the orientation bit string and firstly choose the one with the most same orientation to expand. 
\end{abstract}



\section{Motivation}\label{ssec:moti}
% 近邻图之于ANN搜索、为啥厉害
NNG has now become a powerful tools to address ANN search, especially in high dimensions.


%% 近邻图搜索算法的通病：随机IO太多，运算开销较大
However, there is a common drawback in most search algorithms on NNG, they encounter large amount of random I/Os and consume large amount of original distance evaluations (which is very expensive in high dimensions). 
% 原因：主要有三点、
There are mainly three reasons.
\begin{description}
	\item [-] Due to the large volume of the dataset, it is reasonable to store the topology of the NNG apart from the original dataset. The topology of the NNG is a kind of inverted file, where each node maintains all the IDs of its neighbors. When converging on the NNG, the original data of the neighbors are loaded following the needs. This is a space-efficient storage strategy. Instead, one has to store multiple copies of the datasets. However, the disadvantage is obvious, the neighbors can not be stored adjacently on the disk. Therefore, all the load of the neighbor are of random I/O consumptions.
	\item [-] Existing NNG search algorithms do not pay too much attention on random I/O savings.
\end{description}


The reason is that there is no reference that can be used to fastly discriminate the quality of the neighbors of the current node.
A basic operation of NNG convergence is that when locating at a prior node, one needs to load and check the neighbors of the prior node to provide further converge route to the true NNs.

Existing algorithms can be divided into two categories, the greedy ones and the backtrack ones.

to construct further route to converge or 



\textbf{We see how many places we can apply our OSH techniques.}

\section{Introduction}\label{sec:intro}

\section{Background}\label{sec:back}
\subsection{Search algorithms on NNG}\label{ssec:search_algorithm}
There has been a development over the search algorithms on NNG (or $k$-NNG).

\subsubsection{Naive algorithm}\label{sssec:naive}
A naive algorithm is the downhill search algorithm, as shown in Fig. \ref{fig:search_downhill}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{search_downhill.pdf} \\
	\caption{Downhill search algorithm}\label{fig:search_downhill}
\end{figure}



In \cite{KianaFkNNG2011IJCAI}, the authors uses a ``best-first" strategy to search NNs: start from a random point $Y_0$, expand to the best neighbor among its $E$ neighbors, until it reaches a point who is better than any of its neighbor. Collect all encountered $E$ neighbors, find best $K$ ones as the $K$-NN search result. \textbf{The efficiency is very plain, the speedup around 3-16 on a 17k SIFT datasets with $K=30$}. 
\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{search_FkNNG.pdf}
	\caption{NN Search in FkNNG}\label{fig:search_fknng}
\end{figure}


FANNG~\cite{HarwoodD2016FANNG} proposes a backtrack search algorithm, which is some kind of confused, I cannot understand yet. It seems like a greedy strategy, because it will move to a nearer node once found a nearer one. What confuses me is the priority query stores edges instead of nodes. Now I understand, all the edges are sorted according to the distance between the start point and the query.

$>$ Our technique can help to provide a sorted edge list in the ascending order of the distance to the query, in order to speedup the convergence, as shown in Fig. \ref{fig:search_backtrack}. 
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{search_backtrack.pdf}
	\caption{NN Search in FkNNG}\label{fig:search_backtrack}
\end{figure}


NNGPQ~\cite{WangJDNNGPQ2013} uses a combination of kNNG and a bridge NNG. During the search, it uses a priority queue $Q$ of data points. Each time pop out a best one, expand all its neighbors, push them into the $Q$ and also update the ANN result.

$>$ It is concerned that there is a case which we do not take into account. If we only expand neighbors that are closer than the current point $O$, when $O$ is quite near to the query, a neighbor of $O$ which is even further than $O$ can be a true NN. For this case, we say that for all neighbors that are closer to $O$ we consider, 


\section{Our methods}
\section{Sketch of orientation sensitive hashing}\label{osh}

Orientation sensitive, as the term suggests, means that we want to seize the orientation similarity between data points.

\subsection{Definition of orientation}
Suppose there is a reference point $o \in R^d$, given two data points $p$ and $q$ we define the orientation of $p$ w.r.t $q$ as whether $p$ and $q$ located at the same side of the hyperplane $H_{qo}^{\bot}$, i.e.,
\begin{equation}
orient_q(p)=sign(op\cdot oq)
\end{equation}

\begin{figure}
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=0.45\textwidth]{OSH.pdf}\\
	\caption{Sample of orientation hashing}\label{fig:nncompare}
\end{figure}


We first define a function \textit{OSH\_order} that calculates OSH keys for all edges of a node 


\section{Related Works}



\section{Evaluation}



\section{Conclusion}



\subsection*{Acknowledgments}



\bibliographystyle{IEEEtran}
\bibliography{cuiref}


\end{document}
